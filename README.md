# Finish Finder

AI-assisted UFC fight discovery built with Next.js 15, Prisma, and an automated scraping + prediction pipeline. Finish Finder ranks upcoming bouts by entertainment value using live data from a multi-source scraper and OpenAI-generated analysis, then serves the experience via a themed UFC UI.

## Table of Contents
1. [Overview](#overview)
2. [Architecture Snapshot](#architecture-snapshot)
3. [Quickstart](#quickstart)
   - [Prerequisites](#prerequisites)
   - [Setup](#setup)
   - [Run Locally](#run-locally)
   - [Database Tasks](#database-tasks)
   - [Automation Commands](#automation-commands)
4. [Deployment](#deployment)
5. [Documentation](#documentation)
6. [Project Status](#project-status)

## Overview
Finish Finder helps UFC fans pick the most electric fights. The system:
- Scrapes upcoming UFC cards and fighter data from a **multi-source system** (Wikipedia primary ‚Üí Tapology enrichment ‚Üí Sherdog optional).
- Persists the data in PostgreSQL (SQLite for local play).
- Calls OpenAI to score finish probability, fun factor, and risk.
- Delivers a mobile-first, responsive UFC-styled interface with optimized touch targets and sticky fight insights.
- Exports static JSON bundles for GitHub Pages while supporting a dynamic API on Vercel/Supabase.

> ‚ö†Ô∏è **Automated Scraping Status**: **Partially Operational (Under Investigation)**. Enhanced Tapology parsing implemented with 4-strategy fallback system, but complete end-to-end functionality requires further debugging. See [Scraper Issues](#scraper-issues) below.

Core repo pillars:
- **Frontend** ‚Äì Next.js App Router with client components in `src/app` and modular UI widgets under `src/components`.
- **APIs** ‚Äì Prisma-backed routes in `src/app/api`, including `/api/db-events` (event feed), `/api/fighter-image` (scraped imagery, currently placeholder-only), `/api/health` (system health checks), and `/api/performance` (database performance metrics).
- **Data & AI** ‚Äì `src/lib/ai/hybridUFCService.ts` orchestrates scraping and prediction requests. Supporting utilities live in `src/lib`.
- **Automation** ‚Äì Node scripts in `scripts/` schedule scrapes, regenerate predictions, export static JSON, and prep GitHub Pages artifacts.

## Architecture Snapshot
| Slice | Entrypoints | Notes |
| --- | --- | --- |
| UI | `src/app/page.tsx`, `src/components/**` | Mobile-first responsive design. Fetches `/api/db-events` first, falls back to `public/data/events.json`. Adaptive sidebar: prominent on mobile, sticky on desktop. |
| API | `src/app/api/db-events/route.ts`, `src/app/api/fighter-image/route.ts`, `src/app/api/health/route.ts`, `src/app/api/performance/route.ts` | Prisma event feed with JSON safety guards; fighter-image route currently disabled to reduce third-party scraping noise. Health and performance monitoring endpoints for observability. |
| Data Layer | `prisma/schema.prisma`, `prisma/migrations/**` | Runs on SQLite locally and Supabase/Postgres remotely. Includes prediction usage telemetry tables. |
| Scraper & AI | `scripts/automated-scraper.js`, `src/lib/ai/hybridUFCService.ts`, `src/lib/scrapers/*`, `scripts/generate-*.js` | Handles scrape ‚Üí diff ‚Üí persist ‚Üí prediction replays. Wikipedia supplies fight cards; Tapology enriches fighter records (W-L-D). Writes audit logs under `logs/`. |
| Static Export | `scripts/export-static-data.js`, `scripts/prepare-github-pages.js`, `docs/` | Produces GitHub Pages snapshot with `_next` assets and pre-rendered data. |
| Monitoring | `sentry.*.config.ts`, `src/lib/monitoring/logger.ts`, `src/app/admin/`, `src/lib/database/monitoring.ts` | Sentry is wired for client, server, and edge; logger utilities keep console output structured. Database performance monitoring with admin dashboard at `/admin`. |

## Quickstart

### Prerequisites
- Node.js 20.x (or newer 18+ release that supports `fetch`).
- npm 9+
- PostgreSQL database for persistent runs (SQLite is bundled for local exploration).
- OpenAI API key with access to `gpt-4o` (used by prediction helpers).

### Setup
```bash
npm install
cp .env.example .env.local
# populate .env.local with private keys (OpenAI, Sentry, DATABASE_URL, etc.)
```

### Run Locally
```bash
npm run dev               # Start Next.js with Turbopack on http://localhost:3000
```
The app attempts `/api/db-events` first. Without a database it will read from `public/data/events.json`.

**Admin Dashboard**: Access database performance monitoring at `http://localhost:3000/admin` (password: "admin123" in development mode).

To view the static export:
```bash
npm run pages:build       # Regenerates docs/ and public/data snapshots
```

### Database Tasks
```bash
npm run db:push           # Push Prisma schema to the configured database
npm run db:migrate        # Create + apply a local development migration
npm run db:reset          # Reset schema and reseed (destructive)
```
SQLite lives at `prisma/dev.db`. For Postgres deployments, set `DATABASE_URL` in `.env.local` or the host environment before running these commands.

### Scraper E2E (Dev) ‚Äì Wikipedia/Tapology ‚Üí DB
```bash
# 1) Start Postgres (or point DATABASE_URL to an existing DB)
docker compose -f docker-compose.dev.yml up -d postgres

# 2) Apply schema
npm run db:push

# 3) Run scraper (idempotent; validates data before writes)
npm run scraper:check

# 4) Optional: generate AI predictions for upcoming events
OPENAI_API_KEY=... node scripts/ai-predictions-runner.js

# 5) Verify API contract remains stable
curl -s http://localhost:3000/api/db-events | jq '.data.events[0]'
```

Troubleshooting:
- If fights are not created, ensure the DB is reachable and migrations are applied. The scraper validates fights with `eventId` injected prior to persistence.
- If you see duplicate fight ID conflicts from Wikipedia-only sources, update to latest code: Wikipedia fight IDs are event‚Äëscoped to avoid collisions across events.

### Duplicate Event Management
The system includes enhanced deduplication to handle multi-source data conflicts:
```bash
node check-database-duplicates.js    # Analyze database for duplicates
node test-deduplication.js           # Test deduplication algorithm
node fresh-duplicate-check.js        # Quick duplicate analysis
```
For cleanup operations, see `OPERATIONS.md` for detailed instructions.

### Automation Commands
**‚ö†Ô∏è Automated scraping runs daily (Tapology-first with enhanced parsing).**

```bash
npm run scraper:check     # Run scraper locally
npm run scraper:status    # Summarise strike counters and pending predictions
npm run scraper:schedule  # Prepare scheduled execution metadata
npm run predict:event     # Generate AI predictions for the newest event(s)
npm run predict:all       # Regenerate predictions for every tracked fight
npm run pages:build       # Refresh GitHub Pages bundle under docs/
```

Scraper and prediction commands require `DATABASE_URL` and `OPENAI_API_KEY`. They also honour `SCRAPER_CANCEL_THRESHOLD` and `SCRAPER_FIGHT_CANCEL_THRESHOLD` for strike-ledger logic. Logs are written to `logs/scraper.log`, `logs/missing-events.json`, and `logs/missing-fights.json`.

#### Scraper flags
- `SCRAPER_SOURCE`: `tapology|wikipedia` (default `tapology`; controls primary scraping source).
- `SHERDOG_ENABLED`: `true|false` (default `true` locally; CI sets `false`).
- `TAPOLOGY_ENRICH_RECORDS`: `true|false` (default `true` locally and CI; set to `false` to disable).
- `SHERDOG_MAX_RPS`: optional throttle for Sherdog when enabled.

Helper scripts:
- `npm run sherdog:test:local` ‚Äì probe Sherdog org+event pages with rotated headers.
- `node scripts/test-enrich-records.js 1` ‚Äì run 1 event with Tapology record enrichment.
- `node scripts/test-tapology-fighter-record.js "Fighter Name"` ‚Äì fetch a single fighter record from Tapology.

For VPN setup details, see [docker/mullvad/README.md](docker/mullvad/README.md).

## Scraper Issues

### Current Status (2025-09-23)
The automated scraper is experiencing intermittent issues despite significant debugging improvements. Recent work includes:

#### ‚úÖ Implemented Enhanced Tapology Parsing
- **4-Strategy Fallback System**: When standard CSS selectors fail, the scraper now tries 4 progressive strategies:
  1. Elements containing "UFC" + year with comprehensive filtering
  2. Elements with links containing UFC content
  3. Table rows/list items with UFC content
  4. Broad search with size constraints
- **Comprehensive Debugging**: Added extensive console logging to understand HTML structure in real-time
- **Aggressive Link Detection**: Enhanced href pattern matching for Tapology event URLs

#### ‚úÖ Fixed TypeScript Compilation Issues
- Resolved RegExpMatchArray return type conflicts in filter methods
- Ensured CI/CD compatibility for GitHub Actions workflows

#### ‚ö†Ô∏è Remaining Issues
Despite successful URL extraction in testing (events like "UFC Fight Night: Royval vs. Kape" and "UFC 323" were found), the complete end-to-end scraper workflow still reports issues. Investigation ongoing.

#### üîß Recent Changes (Commits fb1ba52, a5a7a09)
- Enhanced `src/lib/scrapers/tapologyService.ts` with debugging and fallback strategies
- Modified GitHub Actions workflow to use Tapology-first configuration
- Added comprehensive logging throughout the parsing pipeline

#### üõ†Ô∏è Debugging Commands
```bash
# Check current scraper status
gh run list --workflow=scraper.yml --limit=5

# View detailed logs from latest run
gh run view --log

# Trigger manual scraper run with limited events
gh workflow run scraper.yml --field events_limit=5

# Local debugging with comprehensive logging
SCRAPER_SOURCE=tapology TAPOLOGY_ENRICH_RECORDS=true npm run scraper:check
```

#### üìã Known Patterns
- Tapology website structure changes frequently, requiring adaptive parsing
- Standard CSS selectors (`.fightcenter_event_listing`, `.event_listing`) often fail
- Alternative parsing strategies successfully find events but integration requires refinement
- GitHub Actions environment may behave differently than local testing

## Deployment
Recommended production topology:
1. **Vercel + Supabase** ‚Äì Deploy the Next.js app to Vercel (`npm run build`) and point Prisma at Supabase Postgres.
2. **Secrets** ‚Äì Configure `DATABASE_URL`, `OPENAI_API_KEY`, `SENTRY_*`, `NEXT_PUBLIC_SENTRY_*`, and scraper thresholds in Vercel env settings and GitHub Actions secrets.
3. **Automated Scraper** ‚Äì Runs in GitHub Actions. CI disables Sherdog (`SHERDOG_ENABLED=false`) and enables Tapology record enrichment (`TAPOLOGY_ENRICH_RECORDS=true`).
4. **Static Mirror (Optional)** ‚Äì After successful scrapes, run `npm run pages:build` and publish `docs/` to GitHub Pages for a static fallback.

### GitHub Actions VPN Setup
```bash
# Configure Mullvad VPN for automated scraping
gh secret set MULLVAD_ACCOUNT_TOKEN --body "your_account_token"

# Optional: customize VPN relay location
gh variable set MULLVAD_RELAY_LOCATION --body "us-nyc-wg-301"
```

See [`ARCHITECTURE.md`](ARCHITECTURE.md) and [`OPERATIONS.md`](OPERATIONS.md) for deeper diagrams, runbooks, and deployment checklists.

## Documentation
- [`ARCHITECTURE.md`](ARCHITECTURE.md) ‚Äì Context, containers, components, and key data flows.
- [`CONTRIBUTING.md`](CONTRIBUTING.md) ‚Äì Branch strategy, code review expectations, and required checks.
- [`OPERATIONS.md`](OPERATIONS.md) ‚Äì Runbooks for scrapes, predictions, migrations, and incident response.
- [`TESTING.md`](TESTING.md) ‚Äì Testing philosophy, coverage targets, and how to add new suites.
- [`STYLEGUIDE.md`](STYLEGUIDE.md) ‚Äì UI, TypeScript, and logging conventions.
- [`SECURITY.md`](SECURITY.md) ‚Äì Threat model, secrets policy, and dependency hygiene.
- [`docs/handbook/README.md`](docs/handbook/README.md) ‚Äì Legacy handbook retained for reference; superseded by the documents above.

## Project Status
- Active prototype with production aspirations; database migrations for Supabase live under `prisma/migrations/`.
- ‚úÖ **TypeScript + ESLint in CI** ‚Äì Use `npx tsc --noEmit` and `npm run lint` in CI for quality gates.
- ‚ö†Ô∏è **Production build on Vercel ignores lint/type errors** ‚Äì To ensure deployment continuity, `next.config.ts` opts out of blocking on lint/type errors. CI should enforce quality before deploys.
- Automated testing is not yet implemented‚Äîquality assured through TypeScript compilation and linting. ROADMAP highlights next steps for test coverage.
- `docs/_next/` and `out/` store large static exports checked into git; consider pruning or generating on-demand for lighter clones.
- Secrets must never be committed. Regenerate any keys that were previously stored in version control and rely on `.env.local` going forward.

## Code Quality
```bash
npm run lint          # ESLint checks - must pass for builds
npx tsc --noEmit      # TypeScript compilation - must pass for builds
npm run build         # Full build with quality gates enabled
```
- **TypeScript**: Strict mode; treat CI as the source of truth for blocking
- **ESLint**: Comprehensive rules; CI should block on violations
- **Build Gates**: Vercel deploys do not block on lint/type errors; use CI to enforce

Planned: Lint/Type Cleanup
- Remove remaining `any`/`require()` usage in scrapers and migrate to idiomatic ESM imports.
- Address ESLint violations to re‚Äëenable blocking builds on Vercel (flip `ignoreBuildErrors` and `ignoreDuringBuilds` back to false).

---
Licensed under the MIT License.
