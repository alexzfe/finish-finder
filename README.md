# Finish Finder

AI-assisted UFC fight discovery built with Next.js 15, Prisma, and an automated scraping + prediction pipeline. Finish Finder ranks upcoming bouts by entertainment value using live data from a multi-source scraper and OpenAI-generated analysis, then serves the experience via a themed UFC UI.

## Table of Contents
1. [Overview](#overview)
2. [Architecture Snapshot](#architecture-snapshot)
3. [Quickstart](#quickstart)
   - [Prerequisites](#prerequisites)
   - [Setup](#setup)
   - [Run Locally](#run-locally)
   - [Database Tasks](#database-tasks)
   - [Automation Commands](#automation-commands)
4. [Deployment](#deployment)
5. [Documentation](#documentation)
6. [Project Status](#project-status)

## Overview
Finish Finder helps UFC fans pick the most electric fights. The system:
- Scrapes upcoming UFC cards and fighter data from **UFCStats.com** using a Python/Scrapy scraper.
- Persists the data in PostgreSQL (SQLite for local play).
- Calls OpenAI to score finish probability, fun factor, and risk.
- Delivers a mobile-first, responsive UFC-styled interface with optimized touch targets and sticky fight insights.
- Exports static JSON bundles for GitHub Pages while supporting a dynamic API on Vercel/Supabase.

> ‚úÖ **Automated Scraping Status**: **Phase 1 Complete**. Python/Scrapy scraper infrastructure deployed with content hash change detection, database schema updated, and ingestion API operational. Scraper implementation (parsers, tests) in progress.

Core repo pillars:
- **Frontend** ‚Äì Next.js App Router with client components in `src/app` and modular UI widgets under `src/components`.
- **APIs** ‚Äì Prisma-backed routes in `src/app/api`, including `/api/db-events` (event feed), `/api/internal/ingest` (scraper ingestion), `/api/health` (system health checks), and `/api/performance` (database performance metrics).
- **Scraper** ‚Äì Python/Scrapy scraper in `/scraper` directory that crawls UFCStats.com and POSTs to the ingestion API with content hash change detection.
- **Data & AI** ‚Äì `src/lib/ai/hybridUFCService.ts` orchestrates prediction requests. Supporting utilities and validation schemas live in `src/lib`.
- **Automation** ‚Äì Node scripts in `scripts/` schedule scrapes, regenerate predictions, export static JSON, and prep GitHub Pages artifacts.

## Architecture Snapshot
| Slice | Entrypoints | Notes |
| --- | --- | --- |
| UI | `src/app/page.tsx`, `src/components/**` | Mobile-first responsive design. Fetches `/api/db-events` first, falls back to `public/data/events.json`. Adaptive sidebar: prominent on mobile, sticky on desktop. |
| API | `src/app/api/db-events/route.ts`, `src/app/api/fighter-image/route.ts`, `src/app/api/health/route.ts`, `src/app/api/performance/route.ts` | Prisma event feed with JSON safety guards; fighter-image route currently disabled to reduce third-party scraping noise. Health and performance monitoring endpoints for observability. |
| Data Layer | `prisma/schema.prisma`, `prisma/migrations/**` | Runs on SQLite locally and Supabase/Postgres remotely. Includes ScrapeLog audit table and scraper fields (sourceUrl, contentHash, lastScrapedAt). |
| Scraper | `scraper/ufc_scraper/`, `src/app/api/internal/ingest/route.ts`, `src/lib/scraper/validation.ts` | Python/Scrapy spider extracts data from UFCStats.com ‚Üí POSTs JSON to Next.js API ‚Üí Transaction-safe upserts with SHA256 change detection. Creates ScrapeLog entries for monitoring. |
| AI | `src/lib/ai/hybridUFCService.ts`, `scripts/generate-*.js` | Generates fight predictions using OpenAI GPT-4o based on scraped data. |
| Static Export | `scripts/export-static-data.js`, `scripts/prepare-github-pages.js`, `docs/` | Produces GitHub Pages snapshot with `_next` assets and pre-rendered data. |
| Monitoring | `sentry.*.config.ts`, `src/lib/monitoring/logger.ts`, `src/app/admin/`, `src/lib/database/monitoring.ts` | Sentry is wired for client, server, and edge; logger utilities keep console output structured. Database performance monitoring with admin dashboard at `/admin`. |

## Quickstart

### Prerequisites
- Node.js 20.x (or newer 18+ release that supports `fetch`).
- npm 9+
- **Python 3.11+** (for web scraper)
- PostgreSQL database for persistent runs (SQLite is bundled for local exploration).
- OpenAI API key with access to `gpt-4o` (used by prediction helpers).

### Setup
```bash
npm install
cp .env.example .env.local
# populate .env.local with private keys (OpenAI, Sentry, DATABASE_URL, etc.)
```

### Run Locally
```bash
npm run dev               # Start Next.js with Turbopack on http://localhost:3000
```
The app attempts `/api/db-events` first. Without a database it will read from `public/data/events.json`.

**Admin Dashboard**: Access database performance monitoring at `http://localhost:3000/admin` (password: "admin123" in development mode).

To view the static export:
```bash
npm run pages:build       # Regenerates docs/ and public/data snapshots
```

### Database Tasks
```bash
npm run db:push           # Push Prisma schema to the configured database
npm run db:migrate        # Create + apply a local development migration
npm run db:reset          # Reset schema and reseed (destructive)
```
SQLite lives at `prisma/dev.db`. For Postgres deployments, set `DATABASE_URL` in `.env.local` or the host environment before running these commands.

### Scraper E2E (Dev) ‚Äì Wikipedia/Tapology ‚Üí DB
```bash
# 1) Start Postgres (or point DATABASE_URL to an existing DB)
docker compose -f docker-compose.dev.yml up -d postgres

# 2) Apply schema
npm run db:push

# 3) Run scraper (idempotent; validates data before writes)
npm run scraper:check

# 4) Optional: generate AI predictions for upcoming events
OPENAI_API_KEY=... node scripts/ai-predictions-runner.js

# 5) Verify API contract remains stable
curl -s http://localhost:3000/api/db-events | jq '.data.events[0]'
```

Troubleshooting:
- If fights are not created, ensure the DB is reachable and migrations are applied. The scraper validates fights with `eventId` injected prior to persistence.
- If you see duplicate fight ID conflicts from Wikipedia-only sources, update to latest code: Wikipedia fight IDs are event‚Äëscoped to avoid collisions across events.

### Duplicate Event Management
The system includes enhanced deduplication to handle multi-source data conflicts:
```bash
node check-database-duplicates.js    # Analyze database for duplicates
node test-deduplication.js           # Test deduplication algorithm
node fresh-duplicate-check.js        # Quick duplicate analysis
```
For cleanup operations, see `OPERATIONS.md` for detailed instructions.

### Python Scraper Setup

```bash
# Install Python dependencies
cd scraper
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Configure environment variables
export INGEST_API_URL="http://localhost:3000/api/internal/ingest"
export INGEST_API_SECRET="your-secret-token"

# Run the scraper
scrapy crawl ufcstats
```

For detailed scraper documentation, see [`scraper/README.md`](scraper/README.md) and [`docs/NEW_SCRAPER_ARCHITECTURE.md`](docs/NEW_SCRAPER_ARCHITECTURE.md).

### Automation Commands

```bash
npm run predict:event     # Generate AI predictions for the newest event(s)
npm run predict:all       # Regenerate predictions for every tracked fight
npm run pages:build       # Refresh GitHub Pages bundle under docs/
```

Prediction commands require `DATABASE_URL` and `OPENAI_API_KEY`.

## Scraper Architecture

### Current Implementation (2025-11-01)

The scraper has been completely rebuilt using Python/Scrapy with a decoupled architecture:

**Architecture:** Python Scrapy Spider ‚Üí Next.js Ingestion API ‚Üí PostgreSQL Database

**Key Features:**
- **Single Data Source**: UFCStats.com exclusively (official UFC stats partner)
- **Content Hash Change Detection**: SHA256-based change detection skips unnecessary database writes
- **Transaction Safety**: All upserts wrapped in Prisma transactions for atomicity
- **Audit Trail**: ScrapeLog table tracks every scraper execution with metrics
- **Authentication**: Bearer token authentication for API security

**Implementation Status:**
- ‚úÖ **Phase 1 Complete**: Database schema, ingestion API, and scraper infrastructure deployed
- üî® **In Progress**: Python spider implementation (parsers, HTML extraction, tests)
- üìã **Planned**: GitHub Actions automation workflow

For complete architecture details, see [`docs/NEW_SCRAPER_ARCHITECTURE.md`](docs/NEW_SCRAPER_ARCHITECTURE.md).

**Archived:** Previous TypeScript multi-source scraper (Wikipedia/Tapology) moved to `/archive/scrapers-old-2025-01/` due to reliability issues and complex deduplication requirements.

## Deployment
Recommended production topology:
1. **Vercel + Supabase** ‚Äì Deploy the Next.js app to Vercel (`npm run build`) and point Prisma at Supabase Postgres.
2. **Secrets** ‚Äì Configure `DATABASE_URL`, `DIRECT_DATABASE_URL`, `INGEST_API_SECRET`, `OPENAI_API_KEY`, and `SENTRY_*` in Vercel env settings and GitHub Actions secrets.
3. **Automated Scraper** ‚Äì Python/Scrapy scraper runs in GitHub Actions, POSTs to `/api/internal/ingest` endpoint.
4. **Static Mirror (Optional)** ‚Äì After successful scrapes, run `npm run pages:build` and publish `docs/` to GitHub Pages for a static fallback.

See [`ARCHITECTURE.md`](ARCHITECTURE.md) and [`OPERATIONS.md`](OPERATIONS.md) for deeper diagrams, runbooks, and deployment checklists.

## Documentation
- [`ARCHITECTURE.md`](ARCHITECTURE.md) ‚Äì Context, containers, components, and key data flows.
- [`CONTRIBUTING.md`](CONTRIBUTING.md) ‚Äì Branch strategy, code review expectations, and required checks.
- [`OPERATIONS.md`](OPERATIONS.md) ‚Äì Runbooks for scrapes, predictions, migrations, and incident response.
- [`TESTING.md`](TESTING.md) ‚Äì Testing philosophy, coverage targets, and how to add new suites.
- [`STYLEGUIDE.md`](STYLEGUIDE.md) ‚Äì UI, TypeScript, and logging conventions.
- [`SECURITY.md`](SECURITY.md) ‚Äì Threat model, secrets policy, and dependency hygiene.
- [`docs/handbook/README.md`](docs/handbook/README.md) ‚Äì Legacy handbook retained for reference; superseded by the documents above.

## Project Status
- Active prototype with production aspirations; database migrations for Supabase live under `prisma/migrations/`.
- ‚úÖ **TypeScript + ESLint in CI** ‚Äì Use `npx tsc --noEmit` and `npm run lint` in CI for quality gates.
- ‚ö†Ô∏è **Production build on Vercel ignores lint/type errors** ‚Äì To ensure deployment continuity, `next.config.ts` opts out of blocking on lint/type errors. CI should enforce quality before deploys.
- Automated testing is not yet implemented‚Äîquality assured through TypeScript compilation and linting. ROADMAP highlights next steps for test coverage.
- `docs/_next/` and `out/` store large static exports checked into git; consider pruning or generating on-demand for lighter clones.
- Secrets must never be committed. Regenerate any keys that were previously stored in version control and rely on `.env.local` going forward.

## Code Quality
```bash
npm run lint          # ESLint checks - must pass for builds
npx tsc --noEmit      # TypeScript compilation - must pass for builds
npm run build         # Full build with quality gates enabled
```
- **TypeScript**: Strict mode; treat CI as the source of truth for blocking
- **ESLint**: Comprehensive rules; CI should block on violations
- **Build Gates**: Vercel deploys do not block on lint/type errors; use CI to enforce

Planned: Lint/Type Cleanup
- Remove remaining `any`/`require()` usage in scrapers and migrate to idiomatic ESM imports.
- Address ESLint violations to re‚Äëenable blocking builds on Vercel (flip `ignoreBuildErrors` and `ignoreDuringBuilds` back to false).

---
Licensed under the MIT License.
