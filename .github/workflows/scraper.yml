name: Automated Scraper

on:
  schedule:
    - cron: '0 */4 * * *'  # Run every 4 hours
  workflow_dispatch:        # Allow manual trigger
    inputs:
      events_limit:
        description: 'Number of events to process (default: 50)'
        required: false
        default: '50'

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Standard timeout for scraping
    permissions:
      contents: read
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Generate Prisma Client
        run: npx prisma generate

      - name: Run automated scraper
        env:
          # Existing environment variables
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          SCRAPER_CANCEL_THRESHOLD: ${{ vars.SCRAPER_CANCEL_THRESHOLD }}
          SCRAPER_FIGHT_CANCEL_THRESHOLD: ${{ vars.SCRAPER_FIGHT_CANCEL_THRESHOLD }}
          SENTRY_DSN: ${{ secrets.SENTRY_DSN }}
          NEXT_PUBLIC_SENTRY_DSN: ${{ secrets.NEXT_PUBLIC_SENTRY_DSN }}

          # Disable Sherdog in CI to avoid GH IP blocks
          SHERDOG_ENABLED: 'false'

          # Enable Tapology win/loss record enrichment
          TAPOLOGY_ENRICH_RECORDS: 'true'
        run: |
          node scripts/automated-scraper.js check

      - name: Check scraper health (if scraper failed)
        if: failure()
        run: |
          echo "Scraper failed. Check logs above for details."
          echo "Common issues: Database connection, API rate limits, or source website changes."
